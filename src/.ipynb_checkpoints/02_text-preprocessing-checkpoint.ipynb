{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18209,
     "status": "ok",
     "timestamp": 1616523762309,
     "user": {
      "displayName": "Nat Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXPV2QclQMau5QR4-Eat2Eo6QE-LAPydToClnK=s64",
      "userId": "15612412735832065156"
     },
     "user_tz": 240
    },
    "id": "0OxB07igVcB4",
    "outputId": "5ac0307f-2006-4f11-db43-8141cdd39671"
   },
   "outputs": [],
   "source": [
    "data_dir   = \"../data/\"\n",
    "kaggle_dir = data_dir + \"Kaggle/\"\n",
    "print(kaggle_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2365,
     "status": "ok",
     "timestamp": 1616523768771,
     "user": {
      "displayName": "Nat Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXPV2QclQMau5QR4-Eat2Eo6QE-LAPydToClnK=s64",
      "userId": "15612412735832065156"
     },
     "user_tz": 240
    },
    "id": "bUnQU0pbVoMy"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 3175,
     "status": "ok",
     "timestamp": 1616523771095,
     "user": {
      "displayName": "Nat Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXPV2QclQMau5QR4-Eat2Eo6QE-LAPydToClnK=s64",
      "userId": "15612412735832065156"
     },
     "user_tz": 240
    },
    "id": "dOdeIZpwWwrM",
    "outputId": "98dcbb6d-881f-40ea-f6a9-50c2cae15f57"
   },
   "outputs": [],
   "source": [
    "kaggle_true_df = pd.read_csv(kaggle_dir + \"True.csv.zip\", compression = \"zip\")\n",
    "kaggle_true_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1616523773973,
     "user": {
      "displayName": "Nat Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXPV2QclQMau5QR4-Eat2Eo6QE-LAPydToClnK=s64",
      "userId": "15612412735832065156"
     },
     "user_tz": 240
    },
    "id": "wpYIk5UsUzWv",
    "outputId": "b9327f24-adb3-418c-eb6a-14bb2aa8b2a7"
   },
   "outputs": [],
   "source": [
    "kaggle_true_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 2557,
     "status": "ok",
     "timestamp": 1616523786155,
     "user": {
      "displayName": "Nat Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXPV2QclQMau5QR4-Eat2Eo6QE-LAPydToClnK=s64",
      "userId": "15612412735832065156"
     },
     "user_tz": 240
    },
    "id": "szdj_grCWzYP",
    "outputId": "9afae622-1289-4a50-96f5-a5b4d111fcad"
   },
   "outputs": [],
   "source": [
    "kaggle_fake_df = pd.read_csv(kaggle_dir + \"Fake.csv.zip\", compression = \"zip\")\n",
    "kaggle_fake_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1616523789860,
     "user": {
      "displayName": "Nat Hawkins",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjXPV2QclQMau5QR4-Eat2Eo6QE-LAPydToClnK=s64",
      "userId": "15612412735832065156"
     },
     "user_tz": 240
    },
    "id": "btzTdyNOU1C5",
    "outputId": "d40b934a-abec-4f70-f9dd-792ce49de844"
   },
   "outputs": [],
   "source": [
    "kaggle_fake_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93jKIQg-W0FR"
   },
   "outputs": [],
   "source": [
    "def remove_unencoded_text(text):\n",
    "    \"\"\"\n",
    "    Removes characters that are not UTF-8 encodable.\n",
    "    \"\"\"\n",
    "    return \"\".join([i if ord(i) < 128 else \"\" for i in text])\n",
    "\n",
    "def is_allowed_word(word, stopwords, min_word_len):\n",
    "    \"\"\"\n",
    "    Checks if word is allowed based on inclusion in stopwords and length.\n",
    "    \"\"\"\n",
    "    stopwords_allowed = word not in stopwords\n",
    "    length_allowed = len(word) >= min_word_len\n",
    "    return stopwords_allowed and length_allowed\n",
    "\n",
    "def preprocess(text, stopwords=set(nltk.corpus.stopwords.words(\"english\")),\n",
    "               stem=True, lemmatize=False, keep_alt_forms=False, min_word_len=1):\n",
    "    '''\n",
    "    Standardized preprocessing of a line of text.\n",
    "    '''\n",
    "\n",
    "    # remove non utf-8 characters\n",
    "    text = remove_unencoded_text(text)\n",
    "\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    # convert all whitespace to spaces for splitting\n",
    "    whitespace_pattern = re.compile(r\"\\s+\")\n",
    "    text = re.sub(whitespace_pattern, \" \", text)\n",
    "\n",
    "    # lowercase the input\n",
    "    text = text.lower()\n",
    "\n",
    "    # split into words\n",
    "    words = text.split(\" \")\n",
    "\n",
    "    # stem and/or lemmatize words\n",
    "    # filtering stopwords, numbers, and word lengths as required\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    if stem and lemmatize:\n",
    "        words = [\n",
    "            [word, stemmer.stem(word), lemmatizer.lemmatize(word)]\n",
    "            for word in words if is_allowed_word(\n",
    "                word, stopwords, min_word_len)]\n",
    "    elif stem:\n",
    "        words = [\n",
    "            [word, stemmer.stem(word)]\n",
    "            for word in words if is_allowed_word(\n",
    "                word, stopwords, min_word_len)]\n",
    "    elif lemmatize:\n",
    "        words = [\n",
    "            [word, lemmatizer.lemmatize(word)]\n",
    "            for word in words if is_allowed_word(\n",
    "                word, stopwords, min_word_len)]\n",
    "    else:\n",
    "        words = [\n",
    "            word for word in words if is_allowed_word(\n",
    "                word, stopwords, min_word_len)]\n",
    "\n",
    "    if stem or lemmatize:\n",
    "        if keep_alt_forms:\n",
    "            # return both original and stemmed/lemmatized words\n",
    "            # as long as stems/lemmas are unique\n",
    "            words = [w for word in words for w in set(word)]\n",
    "        else:\n",
    "            # return only requested stems/lemmas\n",
    "            # if both stemming and lemmatizing, return only lemmas\n",
    "            words = list(zip(*words))[-1]\n",
    "\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M33wCKWPX0PD"
   },
   "outputs": [],
   "source": [
    "labels = np.append(np.ones(len(kaggle_true_df)), np.zeros(len(kaggle_fake_df)), axis = 0) # 1 - true news, 0 - fake news\n",
    "aggregate_df = kaggle_true_df.append(kaggle_fake_df)\n",
    "preprocessed_text = []\n",
    "for i in range(len(labels)):\n",
    "    text_ = aggregate_df.iloc[i,:2].values\n",
    "    try:\n",
    "        title = preprocess(text_[0])\n",
    "    except:\n",
    "        continue ## No title\n",
    "    try:\n",
    "        body = preprocess(text_[1])\n",
    "    except:\n",
    "        continue ## No body\n",
    "    preprocessed_text.append([title, body, labels[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vJRotB7cEv9"
   },
   "outputs": [],
   "source": [
    "processed_dataframe = pd.DataFrame.from_records(preprocessed_text, columns = [\"Title\", \"Body\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yUX3KON0cGcp"
   },
   "outputs": [],
   "source": [
    "processed_dataframe.to_csv(data_dir+\"preprocessed_text_w_labels.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPwOTfun29uQ89HMQIn2tPI",
   "collapsed_sections": [],
   "name": "2 - Text Preprocessing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
